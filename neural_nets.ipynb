{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449ce61d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:12:24.029116Z",
     "start_time": "2022-04-04T04:12:24.015114Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798a38cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:12:25.418280Z",
     "start_time": "2022-04-04T04:12:24.286788Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('/home/ivan/Рабочий стол/vtb-matching/')\n",
    "tqdm_notebook.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72f16e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:12:38.686120Z",
     "start_time": "2022-04-04T04:12:38.674074Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_utils import SiamLikeDataset, train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04033227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:07.663383Z",
     "start_time": "2022-04-03T09:20:07.650235Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e159bd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:12:59.420574Z",
     "start_time": "2022-04-04T04:12:59.408615Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/ivan/Рабочий стол/vtb-matching/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede1e9fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T04:13:00.262422Z",
     "start_time": "2022-04-04T04:13:00.113467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73354, 3), (36678, 3), (36678, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup = pd.read_csv(path.join(data_dir, 'markup.csv'))\n",
    "train, valid, test = train_val_test_split(\n",
    "    markup,test_size=0.25,valid_size=0.25,random_state=42,stratify='target'\n",
    ")\n",
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c79942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:07.307546Z",
     "start_time": "2022-04-03T09:20:07.294930Z"
    }
   },
   "outputs": [],
   "source": [
    "dtst_train = SiamLikeDataset(markup=train,\n",
    "                       transactions_path=path.join(data_dir, 'transaction_data'), \n",
    "                       clickstream_path=path.join(data_dir, 'clickstream_data'))\n",
    "dtst_valid = SiamLikeDataset(markup=valid,\n",
    "                       transactions_path=path.join(data_dir, 'transaction_data'), \n",
    "                       clickstream_path=path.join(data_dir, 'clickstream_data'))\n",
    "dtst_test = SiamLikeDataset(markup=test,\n",
    "                       transactions_path=path.join(data_dir, 'transaction_data'), \n",
    "                       clickstream_path=path.join(data_dir, 'clickstream_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ee3880",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:07.463078Z",
     "start_time": "2022-04-03T09:20:07.450126Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "kwargs = {'num_workers': 0, 'pin_memory': False}\n",
    "train_dataloader = DataLoader(dtst_train, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "valid_dataloader = DataLoader(dtst_valid, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "test_dataloader = DataLoader(dtst_test, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7131dbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:07.861136Z",
     "start_time": "2022-04-03T09:20:07.842091Z"
    }
   },
   "outputs": [],
   "source": [
    "le_mcc = joblib.load(path.join(data_dir, 'models_objects', 'le_mcc'))\n",
    "le_currency_rk = joblib.load(path.join(data_dir, 'models_objects', 'le_currency_rk'))\n",
    "le_click_categories = joblib.load(path.join(data_dir, 'models_objects', 'le_click_categories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a893b3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:08.055005Z",
     "start_time": "2022-04-03T09:20:08.026317Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int = 3):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings, embedding_dim, padding_idx=0),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.emb(x)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size: int, ):\n",
    "        super().__init__()\n",
    "        self.lstm_1d = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_size),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.LSTM(input_size=input_size, hidden_size=64, \n",
    "                    num_layers=1, batch_first=True, \n",
    "                    dropout=0.1, bidirectional=True)\n",
    "        )\n",
    "        self.lstm_2d = nn.Sequential(\n",
    "            nn.LSTM(input_size=input_size, hidden_size=64, \n",
    "                    num_layers=1, batch_first=True, dropout=0.1, bidirectional=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            output, _ = self.lstm_1d(x)\n",
    "        else:\n",
    "            output, _ = self.lstm_2d(x)\n",
    "        return torch.cat((output[:, -1, :64], output[:, 0, 64:]), dim=1)  #актуально для bidirectional\n",
    "\n",
    "\n",
    "class BankModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 mcc_classes: int, mcc_emb_size: int,\n",
    "                 currency_rk_classes: int, currency_rk_emb_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_mcc = EmbeddingModel(num_embeddings=mcc_classes+1, \n",
    "                                      embedding_dim=mcc_emb_size)\n",
    "        self.emb_currency_rk = EmbeddingModel(num_embeddings=currency_rk_classes+1, \n",
    "                                              embedding_dim=currency_rk_emb_size)\n",
    "        self.lstm_mcc = LSTMModel(mcc_emb_size)\n",
    "        self.lstm_currency_rk = LSTMModel(currency_rk_emb_size)\n",
    "        self.lstm_transaction_amt = LSTMModel(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*3, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        mcc_out = self.emb_mcc(x['mcc_code'])\n",
    "        mcc_out = self.lstm_mcc(mcc_out)\n",
    "        \n",
    "        currency_rk_out = self.emb_mcc(x['currency_rk'])\n",
    "        currency_rk_out = self.lstm_mcc(currency_rk_out)\n",
    "        \n",
    "        transaction_amt_out = self.lstm_transaction_amt(torch.unsqueeze(x['transaction_amt'].float(), 2))\n",
    "        \n",
    "        out = torch.cat((mcc_out, currency_rk_out, transaction_amt_out), dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class RTKModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 cat_id_classes: int, cat_id_emb_size: int):\n",
    "        super().__init__()\n",
    "        self.emb_cat_id = EmbeddingModel(num_embeddings=cat_id_classes+1, \n",
    "                                      embedding_dim=cat_id_emb_size)\n",
    "        self.lstm_cat_id = LSTMModel(cat_id_emb_size)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        cat_id_out = self.emb_cat_id(x['cat_id'])\n",
    "        cat_id_out = self.lstm_cat_id(cat_id_out)\n",
    "        out = self.fc(cat_id_out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 mcc_classes: int, mcc_emb_size: int,\n",
    "                 currency_rk_classes: int, currency_rk_emb_size: int, \n",
    "                 cat_id_classes: int, cat_id_emb_size: int):\n",
    "        super().__init__()\n",
    "        self.m_bank = BankModel(mcc_classes=mcc_classes, \n",
    "                                mcc_emb_size=mcc_emb_size, \n",
    "                                currency_rk_classes=currency_rk_classes, \n",
    "                                currency_rk_emb_size=currency_rk_emb_size)\n",
    "        self.m_rtk = RTKModel(cat_id_classes=cat_id_classes, \n",
    "                              cat_id_emb_size=cat_id_emb_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bank_out = self.m_bank(x)\n",
    "        rtk_out = self.m_rtk(x)\n",
    "        return bank_out, rtk_out\n",
    "    \n",
    "    \n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \n",
    "    # https://github.com/adambielski/siamese-triplet/blob/master/losses.py\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, raw=False):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = 0.5 * (target.float() * distances +\n",
    "                        (1 + -1 * target).float() * f.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
    "        if raw:\n",
    "            return losses\n",
    "        else:\n",
    "            return losses.mean() #if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29679683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:08.307829Z",
     "start_time": "2022-04-03T09:20:08.283028Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = CombinedModel(mcc_classes=len(le_mcc.classes_), \n",
    "              mcc_emb_size=3, \n",
    "              currency_rk_classes=len(le_currency_rk.classes_), \n",
    "              currency_rk_emb_size=2, cat_id_classes=len(le_click_categories.classes_), \n",
    "              cat_id_emb_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4acd9eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:20:08.793836Z",
     "start_time": "2022-04-03T09:20:08.759683Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = ContrastiveLoss(1)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2a6f8c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-03T09:37:57.300576Z",
     "start_time": "2022-04-03T09:27:17.076633Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                                             | 0/573 [00:08<?, ?batch/s, loss=0.0339]\n",
      "Epoch 0:   0%|                                                                                                                                                                             | 0/286 [10:31<?, ?batch/s, loss=0.0994]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm_train_dataloader:\n\u001b[1;32m     18\u001b[0m     tqdm_valid_dataloader\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     bank_out, rtk_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     batch_loss \u001b[38;5;241m=\u001b[39m loss(bank_out, rtk_out, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m], raw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     21\u001b[0m     sample_loss\u001b[38;5;241m.\u001b[39mextend(batch_loss\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 113\u001b[0m     bank_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_bank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     rtk_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_rtk(x)\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bank_out, rtk_out\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mBankModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m currency_rk_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_mcc(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrency_rk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m currency_rk_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_mcc(currency_rk_out)\n\u001b[0;32m---> 67\u001b[0m transaction_amt_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_transaction_amt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtransaction_amt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((mcc_out, currency_rk_out, transaction_amt_out), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     70\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_1d(x)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :\u001b[38;5;241m64\u001b[39m], output[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m64\u001b[39m:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Рабочий стол/vtb-matching/venv/lib/python3.9/site-packages/torch/nn/modules/rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    765\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tqdm_train_dataloader:\n",
    "        model.train()\n",
    "        for batch in tqdm_train_dataloader:\n",
    "            tqdm_train_dataloader.set_description(f\"Epoch {epoch}\")\n",
    "            model.zero_grad()\n",
    "            bank_out, rtk_out = model(batch)\n",
    "            batch_loss = loss(bank_out, rtk_out, batch['target'])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            tqdm_train_dataloader.set_postfix(loss=batch_loss.item())\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with tqdm(valid_dataloader, unit=\"batch\") as tqdm_valid_dataloader:\n",
    "            sample_loss = []\n",
    "            for batch in tqdm_train_dataloader:\n",
    "                tqdm_valid_dataloader.set_description(f\"Epoch {epoch}\")\n",
    "                bank_out, rtk_out = model(batch)\n",
    "                batch_loss = loss(bank_out, rtk_out, batch['target'], raw=True).detach()\n",
    "                sample_loss.extend(batch_loss.tolist())\n",
    "                tqdm_valid_dataloader.set_postfix(loss=batch_loss.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ContrastiveLoss(1)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 50\n",
    "\n",
    "train_epoch_losses = []\n",
    "valid_epoch_losses = []\n",
    "es_counter = 0\n",
    "for epoch in range(n_epochs):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tqdm_train_dataloader:\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for batch in tqdm_train_dataloader:\n",
    "            tqdm_train_dataloader.set_description(f\"train Epoch {epoch}\")\n",
    "            model.zero_grad()\n",
    "            bank_out, rtk_out = model(batch)\n",
    "            batch_loss = loss(bank_out, rtk_out, batch['target'], raw=True)\n",
    "            batch_loss.mean().backward()\n",
    "            optimizer.step()\n",
    "            train_loss.extend(batch_loss.detach().tolist())\n",
    "            tqdm_train_dataloader.set_postfix(\n",
    "                batch_loss=batch_loss.mean().item(),\n",
    "                epoch_loss=np.mean(train_loss) if not train_epoch_losses else np.mean(train_epoch_losses))\n",
    "            break\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with tqdm(valid_dataloader, unit=\"batch\") as tqdm_valid_dataloader:\n",
    "            valid_loss = []\n",
    "            for batch in tqdm_train_dataloader:\n",
    "                tqdm_valid_dataloader.set_description(f\"valid Epoch {epoch}\")\n",
    "                bank_out, rtk_out = model(batch)\n",
    "                batch_loss = loss(bank_out, rtk_out, batch['target'], raw=True).detach()\n",
    "                valid_loss.extend(batch_loss.tolist())\n",
    "                tqdm_valid_dataloader.set_postfix(\n",
    "                    batch_loss=batch_loss.mean().item(),\n",
    "                    epoch_loss=np.mean(valid_loss) if not valid_epoch_losses else np.mean(valid_epoch_losses))\n",
    "                break\n",
    "    train_epoch_losses.append(np.mean(train_loss))\n",
    "    valid_epoch_losses.append(np.mean(valid_loss))\n",
    "    now_time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    torch.save(model.state_dict(),\n",
    "               path.join(\n",
    "                   data_dir,\n",
    "                   'nn_chpt',\n",
    "                   f'model_{now_time_str}_{round(train_epoch_losses[-1],5)}_{round(valid_epoch_losses[-1],5)}'))\n",
    "    if (len(valid_epoch_losses) > 1) and (valid_epoch_losses[-1] >= [-2]):\n",
    "        es_counter += 1\n",
    "    elif(len(valid_epoch_losses) > 1) and (valid_epoch_losses[-1] < [-2]):\n",
    "        es_counter = 0\n",
    "    if es_counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61fb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
